"""Mock LLM pipeline tests: Full pipeline with mocked LLM responses.

Verifies that the autonomous agent, action executor, and learning analyzer
work correctly with controlled LLM outputs.
"""

import asyncio
import json
import pytest
from pathlib import Path
from unittest.mock import patch, MagicMock, AsyncMock
from types import SimpleNamespace

import src.brain.autonomous_agent as agent_mod
import src.brain.action_executor as ae_mod
import src.loop.autonomous_loop as loop_mod
from src.brain.action_executor import ActionExecutor, ActionStatus
from src.loop.autonomous_loop import (
    AutonomousLoop,
    LoopTask,
    TaskStatus,
    TaskPriority,
    LearningAnalyzer,
    _parse_llm_json_response,
)


# ── Fixtures ─────────────────────────────────────────────────────

@pytest.fixture()
def executor(tmp_path, monkeypatch):
    """Isolated ActionExecutor."""
    ws = tmp_path / "workspace"
    ws.mkdir()
    data = tmp_path / "data" / "brain"
    data.mkdir(parents=True)

    monkeypatch.setattr(ae_mod, "WORKSPACE_DIR", ws)
    monkeypatch.setattr(ae_mod, "DATA_DIR", data)
    monkeypatch.setattr(ae_mod, "PROJECT_ROOT", tmp_path)
    monkeypatch.setattr(ae_mod, "_LLM_AVAILABLE", False)

    ex = ActionExecutor()
    ex.workspace = ws
    ex.history_file = data / "action_history.jsonl"
    ex.allowed_roots = [ws, data, tmp_path / "src"]
    return ex


@pytest.fixture()
def loop(tmp_path, monkeypatch):
    """Isolated AutonomousLoop."""
    monkeypatch.setattr(loop_mod, "_LLM_AVAILABLE", False)
    return AutonomousLoop(data_dir=str(tmp_path / "loop"))


# ── Mock LLM → Executor pipeline ────────────────────────────────

class TestMockLLMPipeline:
    """Test full pipeline with mock LLM providing instructions."""

    def test_llm_instructs_write_file(self, executor, monkeypatch):
        """Simulate LLM returning a write_file action, executor runs it."""
        # Simulate LLM response that would be parsed by agent
        llm_response = {
            "action_type": "write_file",
            "target": "workspace/llm_output.txt",
            "params": {"content": "Generated by mock LLM"},
        }

        result = executor.execute(
            llm_response["action_type"],
            {"path": llm_response["target"], **llm_response["params"]},
        )
        assert result.status == ActionStatus.SUCCESS

        # Verify file was actually written
        written = (executor.workspace / "llm_output.txt").read_text()
        assert "Generated by mock LLM" in written

    def test_llm_instructs_run_python(self, executor):
        """Simulate LLM returning Python code to execute."""
        llm_code = "result = sum(range(10))\nprint(f'Sum: {result}')"

        result = executor.execute("run_python", {"code": llm_code})
        assert result.status == ActionStatus.SUCCESS
        assert "45" in result.output

    def test_llm_instructs_dangerous_action_blocked(self, executor):
        """LLM tries to execute dangerous command → policy blocks it."""
        result = executor.execute("run_shell", {"command": "rm -rf /"})
        assert result.status == ActionStatus.FAILED
        assert result.policy_blocked is True

    def test_llm_chain_read_edit_verify(self, executor):
        """Multi-step chain: write → read → edit → verify."""
        # Step 1: Write initial file
        executor.execute("write_file", {
            "path": "workspace/chain.txt",
            "content": "Hello World",
        })

        # Step 2: Read it back
        r = executor.execute("read_file", {"path": "workspace/chain.txt"})
        assert r.status == ActionStatus.SUCCESS
        assert "Hello World" in r.output

        # Step 3: Edit (simulated LLM edit instruction)
        e = executor.execute("edit_file", {
            "path": "workspace/chain.txt",
            "old": "Hello World",
            "new": "Hello NEXUS",
        })
        assert e.status == ActionStatus.SUCCESS

        # Step 4: Verify edit
        v = executor.execute("read_file", {"path": "workspace/chain.txt"})
        assert "Hello NEXUS" in v.output


# ── Mock LLM → Loop integration ─────────────────────────────────

class TestMockLLMLoopIntegration:
    """Test AutonomousLoop with mock LLM for task planning."""

    def test_loop_executes_llm_planned_task(self, loop):
        """Loop task with run_command executes through action dispatch."""
        task = LoopTask(
            id="mock_1", name="llm_task", description="LLM planned echo",
            action="run_command", params={"command": "echo mock_llm_output"},
        )
        result = asyncio.run(loop.execute_task(task))
        assert result["success"]
        assert "mock_llm_output" in result["output"]["stdout"]
        assert task.status == TaskStatus.COMPLETED

    def test_loop_handles_llm_invalid_action(self, loop):
        """Loop handles LLM suggesting an unknown action gracefully."""
        task = LoopTask(
            id="mock_2", name="bad_action", description="LLM bad suggestion",
            action="teleport_to_mars", params={},
        )
        result = asyncio.run(loop.execute_task(task))
        assert not result["success"]
        assert "Unknown action" in result.get("error", "")

    def test_loop_task_retry_on_failure(self, loop):
        """Task fails → retries → eventually succeeds or exhausts."""
        task = LoopTask(
            id="mock_3", name="retry_task", description="test retry",
            action="run_command", params={"command": "echo ok; bad_meta"},
            max_retries=2,
        )
        result = asyncio.run(loop.execute_task(task))
        assert not result["success"]
        assert task.retry_count == 1
        assert task.status == TaskStatus.PENDING  # still has retries

    def test_loop_multiple_tasks_execution_order(self, loop):
        """Multiple tasks execute in priority order."""
        loop.add_task("low_task", "desc", "run_command",
                       params={"command": "echo low"},
                       priority=TaskPriority.LOW)
        loop.add_task("critical_task", "desc", "run_command",
                       params={"command": "echo critical"},
                       priority=TaskPriority.CRITICAL)
        loop.add_task("medium_task", "desc", "run_command",
                       params={"command": "echo medium"},
                       priority=TaskPriority.MEDIUM)

        # Check queue ordering
        names = [t.name for t in loop.task_queue]
        critical_idx = names.index("critical_task")
        low_idx = names.index("low_task")
        assert critical_idx < low_idx


# ── LLM JSON response parsing pipeline ──────────────────────────

class TestLLMResponseParsing:
    """Test the full LLM response → parsed action pipeline."""

    def test_parse_action_from_plain_json(self):
        """LLM returns clean JSON → parse successfully."""
        llm_output = '{"action": "write_file", "path": "test.txt", "content": "hello"}'
        parsed = _parse_llm_json_response(llm_output)
        assert parsed["action"] == "write_file"

    def test_parse_action_from_markdown_block(self):
        """LLM wraps JSON in markdown code block → parse successfully."""
        llm_output = '```json\n{"action": "run_python", "code": "print(42)"}\n```'
        parsed = _parse_llm_json_response(llm_output)
        assert parsed["action"] == "run_python"

    def test_parse_action_from_chatty_response(self):
        """LLM has surrounding text → extract embedded JSON."""
        llm_output = 'I suggest this action: {"action": "read_file", "path": "main.py"} which reads the file.'
        parsed = _parse_llm_json_response(llm_output)
        assert parsed["action"] == "read_file"

    def test_parse_chain_then_execute(self, executor):
        """Parse LLM response → execute through ActionExecutor."""
        llm_responses = [
            '{"action": "write_file", "path": "workspace/step1.txt", "content": "data"}',
            '{"action": "read_file", "path": "workspace/step1.txt"}',
        ]

        for resp in llm_responses:
            parsed = _parse_llm_json_response(resp)
            action = parsed.pop("action")
            if "path" in parsed and not parsed["path"].startswith("workspace/"):
                parsed["path"] = f"workspace/{parsed['path']}"
            result = executor.execute(action, parsed)
            assert result.status == ActionStatus.SUCCESS


# ── Learning Analyzer with mock LLM ─────────────────────────────

class TestLearningAnalyzerMockLLM:
    """Test LearningAnalyzer produces insights from task results."""

    def test_failure_analysis_without_llm(self, monkeypatch):
        monkeypatch.setattr(loop_mod, "_LLM_AVAILABLE", False)
        analyzer = LearningAnalyzer()

        task = LoopTask(
            id="la_1", name="failed_deploy", description="deploy app",
            action="run_command", params={"command": "deploy"},
        )
        task.result = {"success": False, "error": "permission denied"}
        verification = {"success": False, "error": "permission denied"}

        analysis = analyzer.analyze_result(task, verification)
        assert len(analysis["learnings"]) >= 1
        assert analysis["learnings"][0]["type"] == "failure_pattern"

    def test_success_analysis_without_llm(self, monkeypatch):
        monkeypatch.setattr(loop_mod, "_LLM_AVAILABLE", False)
        analyzer = LearningAnalyzer()

        task = LoopTask(
            id="la_2", name="build_ok", description="build project",
            action="run_command", params={"command": "echo build"},
        )
        task.result = {"success": True}
        verification = {"success": True}

        analysis = analyzer.analyze_result(task, verification)
        success_learnings = [l for l in analysis["learnings"] if l["type"] == "success_pattern"]
        assert len(success_learnings) >= 1

    def test_retry_pattern_flagged(self, monkeypatch):
        monkeypatch.setattr(loop_mod, "_LLM_AVAILABLE", False)
        analyzer = LearningAnalyzer()

        task = LoopTask(
            id="la_3", name="flaky_task", description="flaky test",
            action="run_command", params={}, retry_count=3,
        )
        task.result = {"success": True}
        verification = {"success": True}

        analysis = analyzer.analyze_result(task, verification)
        retry_learnings = [l for l in analysis["learnings"] if l["type"] == "retry_pattern"]
        assert len(retry_learnings) >= 1
